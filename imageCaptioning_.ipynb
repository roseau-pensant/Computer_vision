{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyORI7a40qr0hJTyanHjgc1l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!wget https://postechackr-my.sharepoint.com/:u:/g/personal/dongbinna_postech_ac_kr/EXVy7_7pF5FIsPp6WfXXfWgBNfUKx8N1VrTisN8FbGYG9w?download=1 -O Flickr8k_dataset.zip"],"metadata":{"id":"_Iwr4bavuDls"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!unzip Flickr8k_dataset.zip -d ./Flickr8k_dataset"],"metadata":{"id":"ewW-ebvnyKMW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import files\n","myfile=files.upload()"],"metadata":{"id":"5eLCrAXl4VMM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","\n","#이미지 데이터셋 불러오기\n","images_path = \"./Flickr8k_dataset/Images\"\n","#train, val, test 이미지 저장, 구글드라이브\n","images_train = '/content/gdrive/MyDrive/data/cv_test/train'\n","images_val = '/content/gdrive/MyDrive/data/cv_test/val'\n","images_test = '/content/gdrive/MyDrive/data/cv_test/test'\n","size=[256, 256]\n","\n","#이미지 크기 재조정\n","def resizeImage(image, size):\n","  return cv2.resize(image, size)\n","\n","#이미지 크기 재조정하여 train, val, test 폴더로 저장\n","if not os.path.exists(images_train):\n","  os.makedirs(images_train)\n","if not os.path.exists(images_val):\n","  os.makedirs(images_val)\n","if not os.path.exists(images_test):\n","  os.makedirs(images_test)\n","\n","images = (os.listdir(images_path))\n","imageNum = len(images)\n","train_imageNum = 6000\n","val_imageNum = 1000\n","\n","#train,val,test 이미지 나눠서 저장하기\n","for i, image in enumerate(images):\n","  if(i+1)<=train_imageNum:\n","    output=images_train\n","  if(i+1)<=train_imageNum+val_imageNum:\n","    output=images_val\n","  else:\n","    output=images_test\n","  with open(os.path.isfile(images_path)) as f:\n","    with open(f) as img:\n","      img = resizeImage(img, size)\n","      img.save(os.path.join(output, image), img.format)"],"metadata":{"id":"ynIpD7oLhTwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pickle\n","import nltk\n","from collections import Counter\n","\n","nltk.download('punkt')\n","\n","caption_path = \"./Flickr8k_dataset/captions.txt\"\n","vocab_path = \"./vocab.pkl\"\n","\n","word_threshold = 5\n","train_caption_path = \"./resized_train/captions.txt\"\n","val_caption_path = \"./resized_val/captions.txt\"\n","test_caption_path = \"./resized_test/captions.txt\"\n","\n","\n","class Vocabulary(object):\n","  \n","    def __init__(self):\n","        self.word = {} #단어\n","        self.idx = {} #인덱스\n","        self.idx = 0\n","\n","    def add_word(self, word):\n","        if not word in self.word:\n","            self.word[word] = self.idx\n","            self.idx[self.idx] = word\n","            self.idx += 1\n","\n","    def __call__(self, word):\n","        if not word in self.word:\n","            return self.word['<unk>'] #단어가없다면\n","        return self.word[word]\n","\n","    def __len__(self):\n","        return len(self.word)\n","\n","counter = Counter()\n","\n","with open(caption_path, \"r\") as f:\n","    lines = sorted(f.readlines()[1:])\n","    for i in range(len(lines)):\n","        line = lines[i]\n","        if (i + 1) <= train_imageNum * 5: #이미지당 캡션 5개\n","            output_caption = train_caption_path\n","        elif (i + 1) <= (train_imageNum + val_imageNum) * 5:\n","            output_caption = val_caption_path\n","        else:\n","            output_caption = test_caption_path\n","        index = line.find(\",\") #캡션시작\n","        caption = line[index + 1:] # 캡션(caption) 문자열 기록\n","        tokens = nltk.tokenize.word_tokenize(caption.lower()) #토큰화\n","        counter.update(tokens) #토큰갯수\n","        with open(output_caption, \"a\") as output_caption_f:\n","            output_caption_f.write(line)\n","\n","# 단어의 빈도수가 5이상인 경우에만 사용\n","words = [word for word, cnt in counter.items() if cnt >= word_threshold]\n","\n","vocab = Vocabulary()\n","vocab.add_word('<pad>')\n","vocab.add_word('<start>')\n","vocab.add_word('<end>')\n","vocab.add_word('<unk>')\n","\n","for word in words:\n","    vocab.add_word(word)\n","\n","with open(vocab_path, 'wb') as f:\n","    pickle.dump(vocab, f)"],"metadata":{"id":"oDSN1Mcn9QfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.utils.data as data\n","\n","\n","# Flickr8k 데이터셋\n","class Flickr8kDataset(data.Dataset):\n","    def __init__(self, root, captions, vocab, transform=None):\n","        self.root = root\n","        with open(captions, \"r\") as f:\n","             lines = f.readlines()\n","             self.captions = []\n","             for line in lines: # 첫 번째 줄부터 바로 캡션 정보 존재\n","                index = line.find(\",\") # 캡션(caption) 문자열의 시작점 찾기\n","                path = line[:index] # 이미지 파일 이름\n","                caption = line[index + 1:] # 캡션(caption) 문자열 기록\n","                self.captions.append((path, caption))\n","        self.vocab = vocab\n","        self.transform = transform\n","\n","    # 이미지와 캡션 꺼내기\n","    def __getitem__(self, index):\n","        vocab = self.vocab\n","        path = self.captions[index][0]\n","        caption = self.captions[index][1]\n","\n","        image = open(os.path.join(self.root, path)).convert('RGB')\n","        if self.transform is not None:\n","            image = self.transform(image)\n","\n","        # 캡션(caption) 문자열을 토큰 형태로 바꾸기\n","        tokens = nltk.tokenize.word_tokenize(str(caption).lower())\n","        caption = []\n","        caption.append(vocab('<start>'))\n","        caption.extend([vocab(token) for token in tokens])\n","        caption.append(vocab('<end>'))\n","        target = torch.Tensor(caption)\n","        return image, target\n","\n","    def __len__(self):\n","        return len(self.captions)"],"metadata":{"id":"yFUd9mmHIRga"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n","from torch.nn.utils.rnn import pack_padded_sequence\n","\n","\n","class EncoderCNN(nn.Module):\n","    def __init__(self, embed_size):\n","        # 사전 학습된(pre-trained) ResNet-101을 불러와 FC 레이어를 교체\n","        super(EncoderCNN, self).__init__()\n","        resnet = models.resnet101(pretrained=True)\n","        modules = list(resnet.children())[:-1] # 마지막 FC 레이어를 제거\n","        self.resnet = nn.Sequential(*modules)\n","        self.linear = nn.Linear(resnet.fc.in_features, embed_size) # 결과(output) 차원을 임베딩 차원으로 변경\n","        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n","\n","    def forward(self, images):\n","        # 입력 이미지에서 특징 벡터(feature vectors)\n","        with torch.no_grad(): # 네트워크의 앞 부분은 변경되지 않도록 하기\n","            features = self.resnet(images)\n","        features = features.reshape(features.size(0), -1)\n","        features = self.bn(self.linear(features))\n","        return features\n","\n","\n","class DecoderRNN(nn.Module):\n","    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n","        # 하이퍼 파라미터(hyper-parameters) 설정 및 레이어 생성\n","        super(DecoderRNN, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        self.max_seg_length = max_seq_length"],"metadata":{"id":"lplypPD8LiSH"},"execution_count":null,"outputs":[]}]}